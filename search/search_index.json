{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Blog","text":""},{"location":"2024/02/27/auto-generating-package-api-with-mkdocstrings/","title":"Auto-generating package API with <code>mkdocstrings</code>","text":"<p>Whenever I land on a new Python package docs these days, the docs tend to be built with <code>mkdocs</code> and the <code>mkdocs-material</code> theme, heralding a bit of a departure from the era of the Sphinx.</p> <p>It's true, Sphinx still remains very popular and is endlessly extensible through its directives feature and its many plugins, but there is something about the beauty of <code>mkdocs-material</code>, its ease of deployment and its richness in features that makes it such a popular choice right now.</p> <p>Not to forget the simplicity of writing content in markdown which is very widely understood. All good reasons why I started this blog with <code>mkdocs-material</code>!</p> <p>One thing that Sphinx does very well is autogenerating your project's API using the information present in docstrings. This doesn't come out of the box with <code>mkdocs</code>, but can be enabled with the plugin <code>mkdocstrings</code> which has some great features.</p> <p>The main usage of <code>mkdocstrings</code> seems to be centred around the concept of inline injection, but I wanted to test out their automatic API docs generation feature which they provide a recipe for here.</p>"},{"location":"2024/02/27/auto-generating-package-api-with-mkdocstrings/#setting-up","title":"Setting up","text":"<p>So there's not too much too it - only 4 steps:</p> <ol> <li> <p>Add and install the docs dependencies</p> pyproject.toml<pre><code>[tool.poetry.group.docs.dependencies]\nmkdocs = \"^1.5.3\"\nmkdocs-material = \"^9.5.9\"\nmkdocstrings = {extras = [\"python\"], version = \"^0.24.0\"}\nmkdocs-gen-files = \"^0.5.0\"\nmkdocs-literate-nav = \"^0.6.1\"\nmkdocs-section-index = \"^0.3.8\"\n</code></pre> </li> <li> <p>Add these plugin details to <code>mkdocs.yml</code></p> mkdocs.yml<pre><code>plugins:\n- search\n- gen-files:\n    scripts:\n    - scripts/gen_ref_pages.py\n- literate-nav:\n    nav_file: SUMMARY.md\n- section-index\n- mkdocstrings\n</code></pre> </li> <li> <p>Add the reference section to the navigation settings in <code>mkdocs.yml</code> mkdocs.yml<pre><code>nav:\n- Home: index.md\n...\n- API Reference: reference/\n</code></pre></p> </li> <li> <p>Add the <code>gen_ref_pages.py</code> script to the <code>scripts/</code> folder at the top level</p> </li> </ol>"},{"location":"2024/02/27/auto-generating-package-api-with-mkdocstrings/#trying-it-out","title":"Trying it out","text":"<p>To test this out I created a dummy package, and for a bit of fun I centred it on a Gandalf class \ud83e\uddd9\u200d\u2642\ufe0f.</p> The Gandalf Class <pre><code>\"\"\"Defines the main Gandalf class.\"\"\"\nfrom typing import Literal, get_args\n\nfrom .utils import Mount\n\nWEAPONS = Literal[\"Glamdring\", \"Narya\", \"Staff\"]\nCOLOURS = Literal[\"white\", \"grey\"]\n\n\nclass Gandalf:\n    \"\"\"A Gandalf class.\n\n    Attributes:\n        colour: The colour of Gandalf's robes. Defaults to 'grey'.\n    \"\"\"\n\n    colour: str = \"grey\"\n\n    def __init__(self, weapon: WEAPONS = \"Staff\"):\n        \"\"\"Initialises Gandalf.\n\n        Attributes:\n            weapon: The weapon that Gandalf wields, defaults to 'Staff'.\n            mount: Gandalf's current steed, defaults to None.\n        \"\"\"\n        self._weapon = weapon\n        self._mount = None\n\n    @classmethod\n    def set_colour(cls, colour: COLOURS):\n        \"\"\"Set Gandalf's colour.\"\"\"\n        if colour not in get_args(COLOURS):\n            raise ImproperGandalfColourError\n        cls.colour = colour\n\n    @property\n    def weapon(self):\n        \"\"\"The weapon property.\"\"\"\n        return self._weapon\n\n    @weapon.setter\n    def weapon(self, weapon: WEAPONS):\n        \"\"\"Setter for the weapon property.\"\"\"\n        if weapon not in get_args(WEAPONS):\n            raise WrongWeaponError\n        self._weapon = weapon\n\n    @property\n    def mount(self):\n        \"\"\"Gandalf's mount.\"\"\"\n        return self._mount\n\n    @mount.setter\n    def mount(self, mount: Mount):\n        \"\"\"Setter for the mount property.\"\"\"\n        self._mount = mount\n\n    def deny(self, verb: str) -&gt; None:\n        \"\"\"Shout a denial of a doing word.\n\n        Args:\n            verb: An action word to deny someone of.\n        \"\"\"\n        if not self.weapon == \"Staff\":\n            raise NeedStaffToDenyError(\"Gandalf doesn't have weapon set to 'Staff'.\")\n        print(f\"YOU SHALL NOT {verb.upper()}!!!\")\n\n    def travel(self) -&gt; None:\n        \"\"\"Ride mount to destination.\"\"\"\n        if not self.mount:\n            raise NoMountSetError(\"Gandalf needs a mount to travel.\")\n        self.mount.ride()\n\n\nclass NoMountSetError(Exception):\n    \"\"\"Raise when no mount is set for Gandalf..\"\"\"\n\n\nclass NeedStaffToDenyError(Exception):\n    \"\"\"Raise when Gandalf tries to deny without his staff.\"\"\"\n\n\nclass ImproperGandalfColourError(Exception):\n    \"\"\"Raise when user tries to set Gandalf to the wrong colour.\"\"\"\n\n\nclass WrongWeaponError(Exception):\n    \"\"\"Raise when user tries to set a wrong weapon for Gandalf.\"\"\"\n\n    def __str__(self):\n        return f\"Chosen weapon must be one of {get_args(WEAPONS)}\"\n\n\nif __name__ == \"__main__\":\n    gandalf = Gandalf()\n    gandalf.deny(\"glamp\")\n    gandalf.weapon = \"Glamdring\"\n    gandalf.deny(\"pass\")\n</code></pre> <p>I devised a few rules around setting Gandalf's weapon and his mount. And I gave him two methods for things that he absolutely loves to do:</p> <ul> <li>deny (because Gandalf loves telling you that you shall not do something)</li> <li>travel (because he gets about the map blimming fast)</li> </ul> <p></p> <p>I tried to use a variety of different techniques to see how they generated in the autodocs, such as methods, properties and class methods.</p> <p>I also created two classes with parent ABC <code>Mount</code> in the utils.py, one for ShadowFax and one for the big eagle that comes to his aid.</p> Gandalf's utils <pre><code>\"\"\"Some Gandalf utils.\"\"\"\nfrom abc import ABC, abstractmethod\n\n\nclass Mount(ABC):\n    \"\"\"A mount for Gandalf.\"\"\"\n\n    @staticmethod\n    @abstractmethod\n    def ride() -&gt; None:\n        \"\"\"Ride the mount.\"\"\"\n\n\nclass Shadowfax(Mount):\n    \"\"\"A fast white pony.\"\"\"\n\n    @staticmethod\n    def ride():\n        print(\"WEEEEEE!\")\n\n\nclass Gwaihir(Mount):\n    \"\"\"A great big f**king eagle.\"\"\"\n\n    @staticmethod\n    def ride():\n        print(\"Whoooooooooosh\")\n</code></pre> <p>So now that we have our watertight Gandalf API, we can see how the docs look once generated.</p>"},{"location":"2024/02/27/auto-generating-package-api-with-mkdocstrings/#doing-it-for-real","title":"Doing it for real","text":"<p>I noticed an open issue on the Dynaconf GitHub page which was tagged as a good first issue. They had migrated over to <code>mkdocs-material</code> but they hadn't yet enabled any autogeneration of their API docs. It was present on their old docs but not on their new docs.</p> <p>What they needed was for <code>mkdocstrings</code> to be setup and some tuning to present their API reference as they wanted. I'm not a frequent contributor to open-source but this seemed simple enough that I could do it and also a decent opportunity to learn something, so I started prepping the demo run and offered to give it a go.</p> <p>Before the maintainers could reply to my offer, I went gun-ho and made a first attempt to build the API reference using the same automated recipe that I'd attempted on my dummy API. However, there was a slight difference in the project structure of the <code>dynaconf</code> project that meant a few changes were needed; the <code>gen_ref_pages.py</code> script assumes the following structure:</p> <pre><code>.\n\u2514\u2500\u2500 src/\n    \u2514\u2500\u2500 dynaconf/\n        \u2514\u2500\u2500 __init__.py\n</code></pre> <p>... but <code>dyanconf</code> doesn't have the <code>src/</code> folder:</p> <pre><code>.\n\u2514\u2500\u2500 dynaconf/\n    \u2514\u2500\u2500 __init__.py\n</code></pre> <p>This requires the following changes to the <code>gen_ref_pages.py</code> script:</p> gen_ref_pages.py<pre><code>\"\"\"Generate the code reference pages and navigation.\"\"\"\nfrom pathlib import Path\n\nimport mkdocs_gen_files\n\nnav = mkdocs_gen_files.Nav()\n\nroot = Path(__file__).parent.parent\nsrc = root / \"dynaconf\"  # (1)!\n\nfor path in sorted(src.rglob(\"*.py\")):  # (2)!\n    module_path = path.relative_to(root).with_suffix(\"\")\n    doc_path = path.relative_to(root).with_suffix(\".md\")  # (3)!\n    full_doc_path = Path(\"reference\", doc_path).resolve()  # (4)!\n\n    parts = tuple(module_path.parts)\n\n    if parts[-1] == \"__init__\":\n        parts = parts[:-1]\n        doc_path = doc_path.with_name(\"index.md\")\n        full_doc_path = full_doc_path.with_name(\"index.md\")\n    elif parts[-1] == \"__main__\":\n        continue\n\n    nav[parts] = doc_path.as_posix()\n\n    with mkdocs_gen_files.open(full_doc_path, \"w\") as fd:\n        identifier = \".\".join(parts)\n        print(\"::: \" + identifier, file=fd)\n\n    mkdocs_gen_files.set_edit_path(full_doc_path, path)\n\nwith mkdocs_gen_files.open(\n    Path(\"reference/SUMMARY.md\").resolve(),  # (5)!\n    \"w\",\n) as nav_file:\n    nav_file.writelines(nav.build_literate_nav())\n</code></pre> <ol> <li>Define project <code>root</code> and <code>src</code> as different variables.</li> <li>Glob the py files from <code>src</code></li> <li>Define <code>module_path</code> and <code>doc_path</code> relative to <code>root</code>.</li> <li>Resolve relative paths - see warning below.</li> <li>Resolve relative paths - see warning below.</li> </ol> <p>Warning</p> <p>If the relative paths were not resolved, the <code>mkdocs-gen-files</code> plugin seemed to place the files outside of the project folder. This caused the <code>mkdocs-git-revision-date-plugin</code> used by <code>dynaconf</code> to throw git errors.</p> <p>These changes allowed the full API reference to be built successfully, and it looked like this in the docs:</p> <p></p>"},{"location":"2024/02/27/auto-generating-package-api-with-mkdocstrings/#simplifying-it","title":"Simplifying it","text":"<p>After going gun-ho and generating the full API for every single module, I had some feedback from the maintainers that it was preferred not to have all these modules generated as many of them do not contain public facing API. Time to scale it back.</p> <p>One thing I noticed was that at the top <code>dynaconf</code> level in the reference, it actually auto-generated everything that was present in the top level API that was defined in the <code>__init__.py</code>:</p> dynaconf/__init__.py<pre><code>__all__ = [\n    \"Dynaconf\",\n    \"LazySettings\",\n    \"Validator\",\n    \"FlaskDynaconf\",\n    \"ValidationError\",\n    \"DjangoDynaconf\",\n    \"add_converter\",\n    \"inspect_settings\",\n    \"get_history\",\n    \"DynaconfFormatError\",\n    \"DynaconfParseError\",\n]\n</code></pre> <p>Note</p> <p>It only includes what has been set in the <code>__all__</code> property, not everything that is imported into <code>__init__.py</code>.</p> <p>With that in mind, we can potentially do away with the recursive generation of sub-modules since in this case we are only interested in exposing the top level API. And therefore the only two steps needed are:</p> <ol> <li>Create a new markdown file in the docs with the following:     docs/api.py<pre><code># dyanconf API\n::: dynaconf\n</code></pre></li> <li>And add the section to the navigation in <code>mkdocs.yml</code> mkdocs.yml<pre><code>nav:\n- Home: index.md\n...\n- Reference:\n    - API: api.md\n</code></pre></li> </ol> <p>We end up with a much more simple solution which does the job well and shows off the awesome power of the <code>mkdocstrings</code> plugin!</p> <p>To get the API docs looking super nice, there's a couple of extra settings that were added in this case:</p> mkdocs.yml<pre><code>plugins:\n  - mkdocstrings:\n      handlers:\n        python:\n          options:\n            show_symbol_type_heading: true # (1)!\n            show_symbol_type_toc: true # (2)!\n            show_root_toc_entry: false # (3)!\n            show_object_full_path: true # (4)!\n</code></pre> <ol> <li>Adds the <code>class</code> symbol in front of the object:    </li> <li>Adds symbols to the table of contents:    </li> <li>Removes the module from the top of the toc - it doesn't correspond to anything on the page:    </li> <li>Adds the full dot path to the object:    </li> </ol> <p>Further CSS customisation was added for the Material theme, as recommended in the mkdocstrings docs. I put the CSS in <code>docs/stylesheets/mkdocstrings.css</code> and added the following to <code>mkdocs.yml</code>:</p> mkdocs.yml<pre><code>extra_css:\n  - stylesheets/mkdocstrings.css\n</code></pre> <p>With that all done my PR was approved and merged \ud83e\uddbe and the resulting docs can be previewed here.</p> <p>There are a few improvements to be made to the docstrings themselves but that's for another time.</p> <p>Anyway, I hope you enjoyed my first post and I hope you have the confidence to go and give <code>mdocstrings</code> a go in your own projects.</p>"},{"location":"2024/03/10/spell-check-git-commit-messages-with-this-pre-commit-hook/","title":"Spell check <code>git</code> commit messages with this <code>pre-commit</code> hook","text":"<p>Us developers are often in a rush to get our commits up and nobody wants to meticulously check for spelling errors each time. However, it can be annoying when you check your commits on GitHub to see one.</p> Argggh \ud83d\ude2b <p>Sure, we can live with a spelling error here and there, especially if we squash commit messages when we merge. But if there's a simple way to fix them, then why not?</p> <p><code>typos</code> is a cracking spell checking tool that has very low false positives and is written in Rust, meaning it can make light work of even the largest of monorepos.</p> <p>This isn't just a tool to check your commit messages - no no - this will spell check your entire project in lightning quick time.</p> <p>I had a go at introducing it in an existing project, <code>dynaconf</code>, and you can see for yourself how well it picked up spelling errors. And I have of course just added it as a <code>pre-commit</code> check for this blog too to keep my posts clear of any dreaded spelling errors.</p>"},{"location":"2024/03/10/spell-check-git-commit-messages-with-this-pre-commit-hook/#tip-details","title":"Tip Details","text":"<p>To get it working for your projects, do the following:</p> <ol> <li>Install pre-commit.    <pre><code>pip install pre-commit\n</code></pre></li> <li>Add the following hooks to your <code>.pre-commit-config.yaml</code>, focusing on the highlighting lines if    you only want to add spell check for commit messages (I recommend both).</li> </ol> <p>.pre-commit-config.yaml<pre><code>default_install_hook_types: [pre-commit, commit-msg]\nrepos:\n  - repo: https://github.com/crate-ci/typos\n    rev: v1.19.0\n    hooks:\n      - id: typos\n        args: []\n        stages: [pre-commit]\n      - id: typos\n        name: commit-msg-typos\n        stages: [commit-msg]\n</code></pre> 3. Install the hooks. (1)    <pre><code>pre-commit install\n</code></pre></p> <ol> <li> <p>You should see:</p> <pre><code>pre-commit installed at .git\\hooks\\pre-commit\npre-commit installed at .git\\hooks\\commit-msg\n</code></pre> </li> </ol> <p>Note</p> <p>The <code>commit-msg-typos</code> hook above will fix any typos by default rather than exiting with a zero error code. If you'd rather, you can make the commit fail and fix the errors yourself. Just repeat the option <code>args: []</code> which will turn off automatic fixing.</p> <p>If you typed your commit message out in an editor like <code>vim</code>, it may seem like you've lost your message when your pre-commit check fails. But don't stress, git stores it in the file <code>.git/COMMIT_EDITMSG</code>. To get back to your message and edit your mistakes, use the following command:</p> <pre><code>git commit --edit --file=.git/COMMIT_EDITMSG\n</code></pre> <p>Or register as an alias of your choice, like this example:</p> <pre><code>git config --global alias.fix-commit 'commit --edit --file=.git/COMMIT_EDITMSG'\n</code></pre> <p>then...</p> <pre><code>git fix-commit\n</code></pre>"},{"location":"2024/03/10/spell-check-git-commit-messages-with-this-pre-commit-hook/#final-remarks","title":"Final Remarks","text":"<p>With this all setup, you should have a new pre-commit hook, <code>commit-msg-typos</code> that runs after you have written your message.</p> <p>The default behaviour where <code>typos</code> fixes your spelling errors will mean that this hook will pass almost every time - it will only fail if there are multiple possible corrections as <code>typos</code> won't know which to pick. In this case, or if you have it set to fail on spelling errors so you can correct yourself, take heed of the note above for an additional tip.</p> <p>And finally, as good as <code>typos</code> is, it will still let some errors pass where it is unsure. For example, the error in the intro example wouldn't actually fail or fix because \"ruff\" is not a typical word (it's the name of a package) and the misspelling is an unknown typo which will be ignored. This is because of the importance <code>typos</code> places around low false positives (see more on the design), so do still take caution with your spelling.</p> And an ironic typo to finish... <p>Have fun catching those typos!</p>"},{"location":"2024/03/25/migrating-to-ruff-from-black-and-flake8/","title":"Migrating to <code>ruff</code> from <code>black</code> and <code>flake8</code>","text":"<p>When it comes to linting and formatting Python code, <code>ruff</code> is the ultimate swiss army knife. So next time you venture out into the wilderness of enterprise code bases leave your <code>black</code>, <code>flake8</code>, <code>isort</code> and <code>pydocstyle</code> at home.</p> <p>You can even forget all those plugin attachments for your <code>flake8</code>. Yes, <code>ruff</code> really does do it all. There may be a few gaps but they're being plugged faster than you can list all the linting and formatting tools you had to have before.</p> <p>And did I mention it was built in rust? This means it's pretty damn fast, even on large monorepo code bases. So this coupled with fewer dependencies being installed on each CI pipeline run could potentially save a lot of time over the course of a long project.</p> <p>But is it painful to switch over? That's what I endeavored to find out by having a go at migrating to <code>ruff</code> in the <code>dynaconf</code> code base. Why? <code>dynaconf</code> wants a way to standardise docstrings which <code>ruff</code> offers - it seemed like a good opportunity to introduce it for its other linting and formatting capabilities before switching on the docstring checks.</p> <p>The aim was to keep the formatting as close as possible to the previous configuration, at least as a start. Here's the resulting PR that was merged (success! \ud83c\udf89), so let me talk you through what I learned.</p> <p>Tip</p> <p>Add <code>ruff</code> to your dev dependencies group so it can be installed and run with your CI pipeline. But if you want to avoid installing it in every single one of your virtual environments, you can use <code>pipx</code>. Installing with <code>pipx</code> will put <code>ruff</code> in an isolated virtual environment and add the binaries into a folder that <code>pipx</code> has appended to your path. This means that you can use CLI tools like <code>ruff</code> anywhere, even in your virtual environments, with a single install.</p>"},{"location":"2024/03/25/migrating-to-ruff-from-black-and-flake8/#migration-notes","title":"Migration Notes","text":""},{"location":"2024/03/25/migrating-to-ruff-from-black-and-flake8/#settings-file","title":"Settings file","text":"<p><code>dynaconf</code> doesn't have a <code>pyproject.toml</code> file in the code base yet, so I decided to implement <code>ruff</code> in its own config file <code>ruff.toml</code>. You can see the difference between the two options here.</p> ruff.toml<pre><code># Always generate Python 3.8-compatible code.\ntarget-version = \"py38\"\nline-length = 79\nexclude = [\"dynaconf/vendor*\"]\n</code></pre>"},{"location":"2024/03/25/migrating-to-ruff-from-black-and-flake8/#line-length","title":"Line length","text":"<p>Line length in the <code>dynaconf</code> code base was set to 79, applying PEP8 more strictly than <code>black</code>'s more forgiving default of 88. I wanted to keep this consistent, and this was easily set at the top level of the config file.</p> <p>There were a couple of changes though which needed a bit of explaining:</p> <ul> <li>This change where a line with an emoji was moved onto a single line whereas   previously it was broken across 3. This is because <code>ruff</code> looks at the Unicode   width rather than the character width. I think this means that the unicode width   of this emoji \ud83c\udf9b\ufe0f is 1, whereas the character length may be the length of this: <code>:control_knobs:</code>.</li> <li>This change where a line with a pragma comment (<code>#type</code>, <code># noqa</code>, etc.) is   moved is explained by this reasoning. Basically <code>ruff</code> doesn't move pragma   comments around as this can change their behaviour.</li> </ul>"},{"location":"2024/03/25/migrating-to-ruff-from-black-and-flake8/#use-extend-select-rather-than-select","title":"Use <code>extend-select</code> rather than <code>select</code>","text":"<p>Using <code>select</code> overwrites the default rules(1) with your specified rules rather than extending them. Therefore, it's better to use <code>extend-select</code> rather than repeating the defaults followed by your specific rules in <code>select</code>.</p> <ol> <li>From ruff docs: <p>By default, Ruff enables Flake8's F rules, along with a subset of the E rules, (<code>[\"E4\", \"E7\", \"E9\", \"F\"]</code>) omitting any stylistic rules that overlap with the use of a formatter, like ruff format or Black.</p> </li> </ol> <p>I used this to specify the various plugins and additional tools that were being used in <code>dynaconf</code> - so <code>ruff</code> has replaced 6 additional packages on top of <code>black</code> and <code>flake8</code>!</p> ruff.toml<pre><code>[lint]\nextend-select =[\n    \"T100\", # https://docs.astral.sh/ruff/rules/#flake8-debugger-t10\n    \"T2\", # https://docs.astral.sh/ruff/rules/#flake8-print-t20\n    \"TD\", # https://docs.astral.sh/ruff/rules/#flake8-todos-td\n    \"UP\", # https://docs.astral.sh/ruff/rules/#pyupgrade-up\n    \"I\", # https://docs.astral.sh/ruff/rules/#isort-i\n    \"N\", # https://docs.astral.sh/ruff/rules/#pep8-naming-n\n]\n</code></pre>"},{"location":"2024/03/25/migrating-to-ruff-from-black-and-flake8/#use-per-file-ignores-rather-than-a-blanket-ignore","title":"Use <code>per-file-ignores</code> rather than a blanket <code>ignore</code>","text":"<p>You also want to avoid nesting config files in sub-directories to implement this - <code>per-file-ignores</code> is the way to go. This feature was available in <code>flake8</code> and it's great to see it ported over. Using this allowed us to be less strict in some areas in the test sub-directories, and to also to ignore some rules in <code>__init__.py</code> files that we would want to apply elsewhere.</p> ruff.toml<pre><code>[lint.per-file-ignores]\n\"*/__init__.py\" = [\n    \"F401\", # (not used import)\n    \"F403\", # (star import `from foo import *`)\n]\n\"tests/*\" = [\n    \"N806\", # Lowercase variables\n    \"N802\", # Lowercase function names\n]\n\"tests_functional/*\" = [\n    \"T2\", # flake8-print\n    \"TD\", # flake8-todos\n    \"N806\", # flake8-todos\n    \"E402\", # (Imports not on top)\n    \"F401\", # (not used import)\n]\n</code></pre> <p>Note</p> <p>I think just <code>__init__.py</code> works in the same way as <code>*/__init__.py</code> here.</p> <p>Initially I implemented the nested config files. If you want to see the difference, you can see what I removed on this commit.</p>"},{"location":"2024/03/25/migrating-to-ruff-from-black-and-flake8/#mimicking-current-import-sort-behaviour-in-dynaconf","title":"Mimicking current import sort behaviour in <code>dynaconf</code>","text":"<p>I added a few configurations to the <code>[lint.isort]</code> section:</p> ruff.toml<pre><code>[lint.isort]\nforce-single-line = true\norder-by-type = false\nrequired-imports = [\"from __future__ import annotations\"]\nknown-first-party = [\"dynaconf\"]\n</code></pre> <ul> <li><code>force-single-line = true</code> because this is what <code>dyanconf</code> had already. It basically forces each   individual import onto its own line rather than bundling them together. Although this results in   more lines, it can result in clearer diffs if imports are added or removed in the future (although   GitHub is quite good now at showing which part of the line has changed).</li> <li><code>order-by-type = false</code> to force pure alphabetical sorting rather than grouping functions, classes   and constants and sorting alphabetically within each group. This was just what was done before.</li> <li><code>required-imports = [\"from __future__ import annotations\"]</code> will add this import to the top of   every file. <code>dynaconf</code> had another tool doing this before.</li> <li><code>known-first-party = [\"dynaconf\"]</code> means that when importing parts of the packages own API in the   test scripts, these imports are sorted as first party rather than being bundled in with the third   party libraries which happens when it's not set.</li> </ul>"},{"location":"2024/03/25/migrating-to-ruff-from-black-and-flake8/#sorting-imports-currently-needs-another-pre-commit-hook","title":"Sorting imports currently needs another pre-commit hook","text":"<p>This part of the <code>ruff</code> docs tells you the options for setting up <code>pre-commit</code> hooks. However, the <code>ruff</code> formatter currently doesn't sort imports out of the box - you have to use this command: <code>ruff check --select I --fix</code>. It does say that a unified command for both linting and formatting is in the works though.</p> .pre-commit-config.yaml<pre><code>repos:\n  - repo: https://github.com/astral-sh/ruff-pre-commit\n  # Ruff version.\n  rev: v0.3.0\n  hooks:\n    # Sort imports.\n    - id: ruff # (2)!\n      name: ruff-sort-imports # (1)!\n      args: [--select, I, --fix]\n    # Run the linter.\n    - id: ruff\n    # Run the formatter.\n    - id: ruff-format\n</code></pre> <ol> <li>This will differentiate it from the following call when the hooks are run. I actually forgot to    add this in the PR!</li> <li>Runs <code>ruff check</code>.</li> </ol> <p>Quite a few hooks were removed in <code>dynaconf</code>'s <code>.pre-commit-config.yaml</code> to be replaced with <code>ruff</code>. You can view the full changes here.</p>"},{"location":"2024/03/25/migrating-to-ruff-from-black-and-flake8/#vertical-whitespace","title":"Vertical whitespace","text":"<p><code>ruff</code> seems to favour adding an extra blank line after module level docstrings which seems to be a bit of a deviation from <code>black</code>.</p>"},{"location":"2024/03/25/migrating-to-ruff-from-black-and-flake8/#other-deviations","title":"Other deviations","text":"<p>There is a good section in the docs highlighting all known deviations from <code>black</code>. And they also have a list of unintentional deviations in their issue tracker, including this one which came up in the <code>dynaconf</code> changes.</p>"},{"location":"2024/03/25/migrating-to-ruff-from-black-and-flake8/#future","title":"Future","text":"<p>Switching on the docstring checks! <code>ruff</code> supports multiple docstring conventions. Most projects I see these days prefer the Google convention, as do I, and here's the simplest setup:</p> ruff.toml<pre><code>[tool.ruff.lint]\n# Enable all `pydocstyle` rules, limiting to those that adhere to the\n# Google convention via `convention = \"google\"`, below.\nextend-select = [\"D\"]\n\n[lint.pydocstyle]\nconvention = \"google\"\n</code></pre> <p>I'm planning to sweep over the <code>Dynaconf</code> code base at some point to apply these rules. I'll put a link to this commit once I've tackled it.</p>"},{"location":"2024/03/25/migrating-to-ruff-from-black-and-flake8/#what-im-watching-out-for","title":"What I'm watching out for","text":"<p>This issue was raised to ask for one call per-line with chained method calls. I think this looks a lot better than the current rules, where something like this would be valid:</p> Example from GitHub user nick4u<pre><code>some_query = (\n  select(whatever.id, x.something)\n  .join(x, x.y == whatever.y)\n  .where(x &gt; 12)\n  .order_by(whatever.id)\n)\n</code></pre>"},{"location":"2024/03/25/migrating-to-ruff-from-black-and-flake8/#wrap-up","title":"Wrap Up","text":"<p>So that's the story of my first <code>ruff</code> migration - it took a bit of documentation diving to try and keep the formatting similar to what it was before but it was worthwhile and I uncovered some great features and learnt what <code>ruff</code> is capable of.</p> <p>Tip</p> <p>A final tip (for VS Code users) is to install the VS Code extension for <code>ruff</code>, make this the default formatter for Python and enable \"Format on Save\". This is great for formatting your code on the go and results in your pre-commit checks passing first time much more often.</p> <p>Have you implemented <code>ruff</code> in any of your projects yet? How are you finding it and are there any other features that you're finding invaluable? Please let me know in the comments!</p>"},{"location":"2024/04/06/creating-a-portable-python-development-environment/","title":"Creating a portable Python development environment","text":"<p>I set out on writing this post with a dream... A dream that whenever I was working in a shell environment, I would always have access to the aliases and commands that I'm used to, and that things would look exactly how I want them to. Whether that be at work, on my personal device or logging in to remote compute environments in the cloud.</p> <p>Luckily I am not the first to have this dream, and through the guidance of Jake Wiesler I've been able to make this dream a reality and set up my own portable development environment.</p> <p>The result is a dotfiles repo that stores all my config files and an install script for <code>bash</code> to get everything setup. As you can see in the README, all that's needed is git and an internet connection to clone the repo, then to run <code>install.sh</code> and I should be all setup and ready to go.</p> <p>I wanted to write about my experience setting it up because I've made a couple of different choices:</p> <ul> <li>I've chosen <code>brew</code> as my cross-platform installer rather than <code>nix</code>.   <code>nix</code> looks cool (it's a whole OS) but <code>brew</code> has been around longer and   still does the job as far as I'm concerned.</li> <li>I wanted to use <code>pyenv</code> and <code>pipx</code> to set everything up for Python development.</li> </ul> <p>I also wanted to touch on some of the other tools I've included and changes I might make in the future.</p>"},{"location":"2024/04/06/creating-a-portable-python-development-environment/#using-stow-to-setup-a-portable-config","title":"Using <code>stow</code> to setup a portable config","text":"<p>GNU's <code>stow</code> (<code>brew install stow</code>) enables you to manage your config files through GitHub and provides some simple commands to symlink these files where they need to be to configure your programs. Because they are symlinked, any changes are still picked up by version control which is great.</p> <p>I don't want to write a full guide here since many good one's already exist - once again I refer you to a Jake Wiesler post which helped me out. Very simply, once you've created your own dotfiles repo, you move the config for each of your apps into named sub-directories with the nesting that you want from your <code>$HOME</code> (or wherever your config files usually exist).</p> <p>I'll just show you the commands that I've put in my <code>install.sh</code> to configure new environments.</p> install.sh<pre><code># Adopt allows stow if .bashrc already exists. We just restore back to what we had with Git.\nstow --adopt bash # (1)!\ngit restore .\nstow vim\nstow git\nstow nu\nsource ~/.bashrc # (2)!\n</code></pre> <ol> <li>There may already be a <code>.bashrc</code>. The <code>--adopt</code> option combines yours with the existing and    creates the symlink. I <code>git restore .</code> in the following line to restore it to the version stored    on GitHub and we are left with the symlink - this is a bit of a workaround.</li> <li>I source my new config to apply the changes straight away.</li> </ol>"},{"location":"2024/04/06/creating-a-portable-python-development-environment/#installing-brew","title":"Installing <code>brew</code>","text":"<p>First I install some basic <code>apt-get</code> dependencies to make sure <code>brew</code> gets installed correctly, and then just install <code>brew</code> in the recommended manner.</p> install.sh<pre><code># Install brew dependencies\nsudo apt-get update\nsudo apt-get install build-essential procps curl file git -y\n\n# Download Brew\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\neval \"$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)\" # (1)!\n</code></pre> <ol> <li>Evaluating this command makes sure that <code>brew</code> is available in the shell so we can call it later in the shell script.</li> </ol>"},{"location":"2024/04/06/creating-a-portable-python-development-environment/#install-packages-with-brew","title":"Install packages with <code>brew</code>","text":"install.sh<pre><code># Installing gcc is recommended\nbrew update\nbrew install \\\n        gcc \\\n        git \\\n        stow \\\n        pyenv \\\n        pipx \\\n        node \\\n        yarn \\\n        keychain \\\n        bat \\\n        nushell \\\n        jandedobbeleer/oh-my-posh/oh-my-posh\n</code></pre> <p>Let me run you through a few of these:</p> <ul> <li><code>gcc</code> is recommended following installation of <code>brew</code></li> <li>We already installed <code>git</code> with <code>apt-get</code>, but installing with <code>brew</code> makes it easier to update   to the latest version.</li> <li><code>stow</code> we will need for the config files as mentioned in the earlier section.</li> <li><code>pyenv</code> and <code>pipx</code> are for my Python setup, they will allow us to install Python and useful CLI   dev tools respectively.</li> <li><code>node</code> and <code>yarn</code> for installing Node.js packages</li> <li><code>keychain</code> which starts an ssh-agent in a long running process that persists between logins. This   means that when an SSH key (with a password!) is setup for pushing and pulling from GitHub, you   don't need to put in your password everytime. This is a bit less secure, but if you're happy to   lose a little you gain some convenience while still staying pretty protected.</li> <li><code>bat</code> - this is <code>cat</code> \"with wings\" (on steroids)</li> <li><code>nushell</code> - a new (\ud83e\udd14) shell that I'm experimenting with. You can see   in the <code>stow</code> section that I stow config for this.</li> <li><code>oh-my-posh</code> - this gives you access to useful and aesthetically pleasing   prompt themes.</li> </ul> Updating <code>git</code> with <code>apt-get</code> as an alternative <p>If you want to stick with <code>apt-get</code> for <code>git</code>, then you need to do the following to get the latest version:</p> <pre><code># Update Git to latest version\nsudo apt-add-repository ppa:git-core/ppa\nsudo apt-get update\nsudo apt-get install git\n</code></pre>"},{"location":"2024/04/06/creating-a-portable-python-development-environment/#installing-python","title":"Installing Python","text":"<p>To install and manage Python version I use <code>pyenv</code>. You may be working on different projects with different Python versions, <code>pyenv</code> provides an easy way to install and switch between the versions you need.</p> <p>To get going, I need a few more dependencies installed before I attempt to install Python with <code>pyenv</code>:</p> install.sh<pre><code># Install Python dependencies in Brew\nbrew install openssl readline sqlite3 xz zlib tcl-tk\n</code></pre> <p>And because I have installed these dependencies with <code>brew</code>, I need to set the C compiler to <code>gcc</code> (which I also installed earlier with <code>brew</code>).</p> install.sh<pre><code># Set the compiler to Brew gcc, pyenv installs will fail without this.\nexport CC=/home/linuxbrew/.linuxbrew/bin/gcc-13\n</code></pre> Alternative: Installing build dependencies with <code>apt</code> <p>I was getting some issues installing some Python versions with <code>brew</code> GCC compiler. Rather than completing the steps above you could install Python's build dependencies with <code>apt</code> instead:</p> <pre><code>sudo apt update; sudo apt install build-essential libssl-dev zlib1g-dev \\\nlibbz2-dev libreadline-dev libsqlite3-dev curl \\\nlibncursesw5-dev xz-utils tk-dev libxml2-dev libxmlsec1-dev libffi-dev liblzma-dev\n</code></pre>"},{"location":"2024/04/06/creating-a-portable-python-development-environment/#installing-python-script","title":"Installing Python script","text":"<p>This bit is probably a bit over-engineered, but I created a bash script to grab the latest available minor versions of Python with the latest patch. The script takes a single positional argument <code>n</code> which indicates how many minor versions you want to go back.</p> <p>For example, as of time of writing, <code>3.12.2</code> is the latest stable release, so if I passed <code>4</code> the following versions would be installed:</p> <pre><code>3.12.2\n3.11.8\n3.10.13\n3.9.18\n</code></pre> <p>It then sets the latest Python version as the global default for <code>pyenv</code>, so <code>3.12.2</code> in this case. I've setup my <code>install.sh</code> script to execute this script with <code>n=2</code>.</p> <p>This was mainly a bit of fun and the chance to learn &amp; practice some bash skills. You can check out the script below:</p> install_python_versions.sh <pre><code>#!/bin/bash\n# Default value\nn_vers=1\n\nshow_help() {\n  echo \"Install the latest patch for last n minor Python versions.\"\n  echo\n  echo \"Usage: \"$(basename \"$0\")\" [-h] n\"\n  echo \"Options:\"\n  echo \"  -h, --help     Display this help message\"\n  echo \"  n              Number of minor versions to install\"\n  exit 0\n}\n\n# Parse command-line options\nwhile getopts \"h\" opt; do\n  case $opt in\n    h) show_help ;;\n    \\?) echo \"Invalid option: -$OPTARG\" &gt;&amp;2; exit 1 ;;\n  esac\ndone\n\n# Replace default with first posarg.\nif ! [ -z \"$1\" ]; then\n  n_vers=$1\nfi\n\n# Update pyenv to make sure it can see the latest versions.\nbrew upgrade pyenv\n# Install the latest patch for last n minor Python versions.\npy_versions=$(pyenv install --list | grep -v '[a-z]' | grep -Po '\\d+\\.\\d+' | uniq | tail -$n_vers | tac)\nfor ver in $py_versions; do pyenv install $(pyenv install --list | grep $ver | grep -v '[a-z]' | tail -1); done\n# Set the global Python as the most recent\nlatest_py=$(pyenv versions | grep -Po '\\d+\\.\\d+\\.\\d+' | tail -1)\npyenv global $latest_py\n</code></pre> <p>It basically works by parsing the output of <code>pyenv install --list</code> to get the latest versions. I also added a <code>help</code> function which works in both the following scenarios:</p> <pre><code>./install_python_versions.sh -h     # (1)!\n./install_python_versions.sh -h 2   # (2)!\n</code></pre> <ol> <li>Without a positional argument.</li> <li>With the positional argument, it will still print help rather than running.</li> </ol>"},{"location":"2024/04/06/creating-a-portable-python-development-environment/#installing-python-cli-dev-tools-with-pipx","title":"Installing Python CLI dev tools with <code>pipx</code>","text":"<p><code>pipx</code> installs and runs your Python apps in isolated environments. The shims for each app are all added to a single location (defaults to <code>~/.local/bin</code>) which is added to your path. Why is this cool?</p> <ul> <li>Install tools once, and use them anywhere - no need to install in every virtual environment.</li> <li>You can upgrade to the latest versions of apps without worrying about causing dependency issues   with you current project packages (<code>pipx upgrade-all</code>)</li> </ul> <p>Following an install of <code>pipx</code> (which I installed with <code>brew</code>), you need to run the following to ensure the shims location is on your path with an aptly named command:</p> <pre><code>pipx ensurepath\n</code></pre> <p>So I end up with the following section in my <code>install.sh</code>:</p> install.sh<pre><code># Install Python tools using pipx.\npipx ensurepath\npipx install black\npipx install typos\npipx install ipython\npipx install ruff\npipx install uvicorn\npipx install cookiecutter\npipx install pre-commit\npipx install mypy\npipx install poetry\npipx install mkdocs\npipx inject mkdocs mkdocs-material[imaging]\npipx inject mkdocs mkdocstrings[python]\npipx inject mkdocs mkdocs-glightbox\npipx install jupyter\npipx install asciinema\n</code></pre> <p>I'm not going to go into each of those tools (feel free to ask in the comments), but I will point out that I'm now able to serve this blog locally to view my changes using the <code>mkdocs serve</code> command, without having to activate a virtual environment first.</p> <p>This is because I've injected the extra dependencies <code>mkdocs</code> needs with <code>pipx inject</code> - another cool feature of <code>pipx</code>.</p>"},{"location":"2024/04/06/creating-a-portable-python-development-environment/#generating-an-ssh-key","title":"Generating an SSH key","text":"<p>The final section of my file just creates an SSH key and prints out the public key with a reminder to add it to my GitHub account.</p> install.sh<pre><code># Generate SSH key for GitHub\nssh-keygen -t ed25519\ncat ~/.ssh/id_ed25519.pub\necho 'Add this public SSH key to GitHub account'\n</code></pre>"},{"location":"2024/04/06/creating-a-portable-python-development-environment/#testing-out-the-full-file","title":"Testing out the full file","text":"<p>With all these steps I have my final <code>install.sh</code> file:</p> install_python_versions.sh <pre><code># Install brew dependencies\napt-get update\napt-get install build-essential procps curl file git -y\n\n# Download Brew\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\neval \"$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)\"\n\n# Installing gcc is recommended\nbrew update\nbrew install \\\n        gcc \\\n        git \\\n        stow \\\n        pyenv \\\n        pipx \\\n        node \\\n        yarn \\\n        keychain \\\n        bat \\\n        nushell \\\n        jandedobbeleer/oh-my-posh/oh-my-posh\n\n# Install Python dependencies in Brew\nbrew install openssl readline sqlite3 xz zlib tcl-tk\n# Set the compiler to Brew gcc, pyenv installs will fail without this.\nexport CC=/home/linuxbrew/.linuxbrew/bin/gcc-13\n\n# Install dependencies for image processing (for mkdocs material)\nbrew install cairo freetype libffi libjpeg libpng zlib\n\n# Adopt allows stow if .bashrc already exists. We just restore\n# back to what we had with Git.\nstow --adopt bash\ngit restore .\nstow vim\nstow git\nstow nu\n\nbrew upgrade pyenv\n./install_python_versions.sh 2\n\n# Install Python tools using pipx.\npipx ensurepath\npipx install black\npipx install typos\npipx install ipython\npipx install ruff\npipx install uvicorn\npipx install cookiecutter\npipx install pre-commit\npipx install mypy\npipx install poetry\npipx install mkdocs\npipx inject mkdocs mkdocs-material[imaging]\npipx inject mkdocs mkdocstrings[python]\npipx inject mkdocs mkdocs-glightbox\npipx install jupyter\npipx install asciinema\n\n# Generate SSH key for GitHub\nssh-keygen -t ed25519\ncat ~/.ssh/id_ed25519.pub\necho 'Add this public SSH key to GitHub account'\n</code></pre> <p>And to test it out I'm going to run it in a fresh Ubuntu container with <code>docker</code>.</p> <pre><code>docker run -it ubuntu bash\n</code></pre> <p>This enters a bash terminal as the root user. To simulate properly, I need to setup a new user with <code>sudo</code> access and switch to that user. I also need to make sure <code>git</code> is installed, before cloning my dotfiles repo and entering the directory. You can see all the steps lain out in my <code>dotfiles</code> repo.</p> <p>Now I can run the whole script and check that it runs without any issues!</p> <pre><code>sudo ./install.sh\n</code></pre>"},{"location":"2024/04/06/creating-a-portable-python-development-environment/#future-considerations","title":"Future considerations","text":"<ul> <li>Splitting installs into smaller scripts so I have more fine-grained control of what's installed.</li> <li>Creating a more lightweight install script where necessary.</li> <li>Creating different versions on different branches to try out new setups or configs.</li> </ul>"},{"location":"2024/04/06/creating-a-portable-python-development-environment/#closing-remarks","title":"Closing remarks","text":"<p>I got onto this while setting up a WSL development environment on my new Windows laptop. It's taken a bit of pain setting up but I've learnt a lot along the way (and gone down some rabbit holes as usual). I'm pretty happy with the outcome and hopefully it's pain that I won't have to repeat.</p> <p>One notable tool that is missing from this installable script is <code>docker</code>. In my setup I've got Docker Desktop hooked up to WSL, which when running starts up a <code>docker</code> engine and puts the app in <code>/usr/bin/docker</code>, so I don't need to install it again. You can see my full Windows with WSL setup by following the link in the first paragraph of this section.</p> <p>I'm interested to hear what you think. How would your install scripts differ from mine? Did you learn anything useful? Please let me know in the comments below.</p>"},{"location":"2024/04/01/chocolatey-vs-scoop-vs-winget---which-windows-package-manager-to-use/","title":"Chocolatey vs. Scoop vs Winget - which Windows package manager to use?","text":"<p>It didn't seem like that long ago that there wasn't much choice when it came to package managers on Windows, but as it finally matures a bit there are actually a few to choose from. Knowing the differences between them and which one to choose can be a little overwhelming at first, so in the process of trying to justify my own choices I thought I would write up this article to give a bit of background, explain the options available and some of the key differences between them.</p>"},{"location":"2024/04/01/chocolatey-vs-scoop-vs-winget---which-windows-package-manager-to-use/#chocolatey","title":"Chocolatey","text":"<p>Chocolatey has been around for a while, (since 2011 apparently), and has been my go to for a while. Chocolatey usually requires you to run installs in a shell with admin privileges, and when it comes to installing community packages, this means that you are often running a Powershell script. This comes with a few security concerns because you are running a community submitted script with admin privileges. Chocolatey even have this to say about their own install script:</p> <p>Please inspect https://community.chocolatey.org/install.ps1 prior to running any of these scripts to ensure safety. We already know it's safe, but you should verify the security and contents of any script from the internet you are not familiar with. All of these scripts download a remote PowerShell script and execute it on your machine. We take security very seriously.</p> <p>You should verify the contents of any script from the internet you are not familiar with... OK, I appreciate that this is good advice and all but really? It's a bit like asking me to read the User Agreement (I would link to Limmy's sketch if I could find it). Life's too short and I'm a naturally trusting person (read: too na\u00efve for the internet).</p> <p>Still, I do take some precautions and employ a little common sense, and I want to be able to trust my package manager without reading every darn script.</p> <p>In 2014, Keivan Beigi (the developer of another Windows package manager: AppGet) wrote this critique: Why Chocolatey is Broken Beyond Any Hope. He also mentions security, but it does seem like Chocolatey have stepped things up since then. Their page on security is pretty extensive, in fact they have this to say:</p> <p>Some folks may state that Chocolatey is insecure. That is based on older information and is incorrect to be stated in that way. Feel free to correct the person with \"You mean Chocolatey used to be insecure, you might want to catch up with the last 3+ years.\"</p> <p>It is correct that there were some major security concerns. However, all known concerns have been corrected and/or have a plan to be resolved (e.g. package signing).</p> <p>So it seems as if the team behind Chocolatey have re-doubled their efforts to refute the security concerns that people used to have. As for the other concerns raised by Keivan, I'm not in a position to say if they've been solved, but Chocolatey seems to still be going strong.</p> <p>One demonstration of the security features at work is when I tried to install Google Chrome just now - the checksum didn't match.</p> <p></p> <p>So what do I do now...? I don't know. Guess I stick with Firefox.</p>"},{"location":"2024/04/01/chocolatey-vs-scoop-vs-winget---which-windows-package-manager-to-use/#winget","title":"Winget","text":"<p><code>winget</code> is Microsoft's own package manager for Windows which sources from the Microsoft Store and the Windows Package Manager Community Repository. It's first commit on the open-source CLI was at the tail end of 2019, and they credit Keivan Beigi for AppGet which \"helped us on the initial project direction\". However, if you read Keivan's side of the story it seems like Microsoft pulled some pretty questionable tactics when engaging with him and how they used his ideas.</p> <p>Alas, Keivan seems to accept the silver linings, that Windows finally has a native package manager which is built on the foundation of his ideas, even if they did treat him like shit. They added the credit a couple of weeks after Keivan's article in May 2020... better late than never I suppose.</p> <p>So despite it's potentially villainous origin story, should you use <code>winget</code>? It's great that it's native, so for my case I've tended to use it for Microsoft native products such as OneDrive and OneNote. There doesn't seem to be a good browser based search for the community packages yet. It seems to work fine but I don't have that much more to say for it currently!</p>"},{"location":"2024/04/01/chocolatey-vs-scoop-vs-winget---which-windows-package-manager-to-use/#scoop","title":"Scoop","text":"<p><code>scoop</code> is the final command line installer for Windows that I want to mention, and it does things a little differently to the other two. It takes its inspiration from Homebrew (Mac/Linux) and aims to be:</p> <p>very scriptable, so you can run repeatable setups to get your environment just the way you like, e.g.: <pre><code>scoop install sudo\nsudo scoop install 7zip git openssh --global\nscoop install aria2 curl grep sed less touch\nscoop install python ruby go perl\n</code></pre></p> <p>They list the features that make it different to Chocolatey and Winget, the main one's for me being:</p> <ul> <li>Install don't require admin rights, since they install just for your user account in <code>~/scoop/</code> by   default.</li> <li>It doesn't pollute your path, instead putting shims to installed programs in a single directory   and making sure that is added to the path (this seems similar to how Homebrew does it).</li> </ul> <p>It also has simpler packaging - on the GitHub README it says:</p> <p>Scoop is an alternative to building an installer (e.g. MSI or InnoSetup) \u2014 you just need to zip your program and provide a JSON manifest that describes how to install it.</p> <p>So these amount to quite a big difference from Chocolatey and Winget which will usually search for the MSI installer files and use that to install in the default locations for apps on Windows (<code>C:/ProgramData/...</code>). This requires admin rights which often need to be confirmed with UAC popups if not running with admin privileges.</p> <p>In terms of when to use it, scoop says that:</p> <p>The apps that install best with Scoop are commonly called \"portable\" apps: i.e. compressed program files that run stand-alone when extracted and don't have side-effects like changing the registry or putting files outside the program directory.</p> <p>I've also seen this:</p> <p>Scoop focuses on open-source, command-line developer tools. The scoop-extras bucket is for non developer tools.</p> <p>And this criteria for including apps in the main bucket.</p> <p>I think I encountered the side effects case when I installed VS Code via scoop. First I had to add the extras bucket since VS Code has a GUI:</p> <pre><code>scoop bucket add extras\n</code></pre> <p>Then when I installed VS Code, it gave me a couple of commands to add keys to registry:</p> <pre><code>scoop install extras/vscode\nreg import \"C:\\Users\\User\\scoop\\apps\\vscode\\current\\install-context.reg\"\nreg import \"C:\\Users\\User\\scoop\\apps\\vscode\\current\\install-associations.reg\"\n</code></pre> <p>But all was still simple enough and easily scripted.</p>"},{"location":"2024/04/01/chocolatey-vs-scoop-vs-winget---which-windows-package-manager-to-use/#conclusion","title":"Conclusion","text":"<p>So scoop, I like scoop. Possible the catchiest of the three in terms of naming, but also the most attractive to me as a developer as it looks and quacks like some of the Linux package managers that I like using. I also like the portable nature of it. If I'm doing Windows developery stuff then it will be my first choice. If you're still unsure about it, see this summary page on why you might use scoop.</p> <p>I'll still install <code>choco</code> and have <code>winget</code> as back ups. <code>winget</code> I've used for installing Microsoft Apps. Some apps just aren't in scoop, such as Docker Desktop, so in that case I've fallen back on <code>choco</code>.</p> <p>Do you have different preferences? Feel free to let me know in the comments below.</p> <p>Also, I'm always learning and may get some things a bit wrong, please point that out in the comments too - thanks!</p>"},{"location":"2024/04/06/setting-up-a-new-windows-laptop-for-python-development/","title":"Setting up a new Windows laptop for Python development","text":""},{"location":"2024/04/06/setting-up-a-new-windows-laptop-for-python-development/#the-backstory","title":"The Backstory","text":"<p>I've been doing a bit more development on my personal laptop lately and I was starting to brush up against the limits of my hardware. I have a Thinkpad X270, and although it comes with the renowned keyboard and a sturdy (not quite) bullet-proof chassis, I was starting to question the other parts of the spec... 8GB of RAM, an Intel i5-62000U processor and only 155GB of usable disk... I quickly ran out of space after setting up Windows Subsystem for Linux, a few versions of Python and some virtual environments.</p> <p>Fair enough, I had a bit of bloat that I could slim down but I didn't want to have to make this a regular occurrence. Also, being a bit of a tab whore I was starting to experience a bit of slowness as I switched contexts between VS Code and my browser. Frankly, I could do with an upgrade. And although it had absolutely zero influence over this decision I was also experiencing a few ping issues when playing Age of Empires II: Definitive Edition online despite good ping and speeds on the internet tests... nope, no sway at all.</p> Wireless speeds... musn't grumble <p>Although I was tempted by some new shiny models, I didn't want to break the bank. And although Macs are touted for development, I will forever be a Windows child - I just can't seem to break through my frustrations with the Apple UX. Plus, with Windows Subsystem for Linux now a thing and Windows 11 actually looking quite sleek I didn't feel the need to finally get over my Macphobia. If I had the money, (I think it would be funny...\ud83c\udfb5), I would have probably went for a souped up Dell XPS 15 after seeing some pretty solid reviews. It seemed to offer good performance for programming and creative workloads while also having a decent graphics card.</p> <p>As it happens, I spotted a refurbed model of the XPS 15 (9570) on ebay which was packing a 1TB SSD and 32GB of RAM for just shy of \u00a3500. This model seemed to review pretty well at the time, and I was happy with this spec at the price point so I decided to purchase. I'm not very tuned in to the world of CPU processors, but it came with an i7-8750H which, while not world-leading, seems to offer a considerable improvement over the i5-62000U of my Thinkpad. And on top of that it came equipped with a Nvidia GTX 1050Ti 4GB graphics card, so if Age of Empires did happen to fire up then I should have fewer online teammates complaining about my lag...</p>"},{"location":"2024/04/06/setting-up-a-new-windows-laptop-for-python-development/#setup-goals","title":"Setup Goals","text":"<p>So it arrived and on unboxing first thoughts were it's a bit chunkier, but then I'm intending to use it mainly as a desktop, and I'll probably hang on to the Thinkpad for a more portable work machine anyway.</p> <p>We're getting onto why I actually wanted to write this post, and that's to run through my development setup on a new Windows machine, mainly so that when I have to go through this again I have a good point to start from, but also to hopefully demonstrate some good practices for creating a productive development experience on Windows.</p> <p>Let me run through a few of my main goals:</p> <ol> <li> <p>Using WSL2 for my main development environment</p> <p>Linux is probably the most common build environment when building applications, and as such, it makes sense to develop in Linux if we can. Windows Subsystem for Linux offers an awesome way to run a Linux environment on Windows without a virtual machine. It's pretty simple to do and it automatically mounts your Windows drive. The WSL extension for VS Code is going to provide a seamless development experience between the Windows and Linux environments.</p> </li> <li> <p>Using virtual environments where possible to keep my base installs unpolluted</p> <p>This should make libraries/tools easy to purge or upgrade and to prevent any base installations from getting corrupted. I also want to keep project environments separate, and only install what I need to with as few duplicate installs as necessary.</p> <p>Two great tools for this in Python are <code>pyenv</code>, which allows for easy installation and switching between different Python versions, and <code>pipx</code>, which installs CLI tools in their own isolated virtual environments, puts all the shims in one place and adds this location to your path so you can use these tools anywhere.</p> </li> <li> <p>Using the command-line and package managers</p> <p>The aim is to automate much of this in the future by putting a bunch of commands into a script, so using the command-line is a must. I also want to make use of some useful package managers:</p> <ul> <li>chocolatey (<code>choco</code>) / scoop / <code>winget</code> for   Windows</li> <li><code>apt</code> for Linux (Debian/Ubuntu) and also Homebrew (<code>brew</code>) (yes it works   on Linux too)</li> </ul> </li> <li> <p>Making my config dotfiles portable</p> <p>As developers, over time we gather a considerable amount of configurations that once we get used to it becomes really difficult to live without. I want a way to make these easily portable to new machines so that even if I'm SSH-ing into a machine on the cloud I can get the development environment that's going to enable me to be most productive.</p> </li> </ol> <p>Anyway, enough goal-setting, let's get cracking...</p>"},{"location":"2024/04/06/setting-up-a-new-windows-laptop-for-python-development/#the-setup","title":"The Setup","text":""},{"location":"2024/04/06/setting-up-a-new-windows-laptop-for-python-development/#installing-wsl","title":"Installing WSL","text":"<p>First on my list is installing WSL. This can take a few hours so it's best to get started with early. Open Powershell with admin privileges and type in the following command:</p> <pre><code>wsl --install\n</code></pre> <p>That easy! The default OS installed for me was Ubuntu 22.04. Leave that to brew while we move onto some of our Windows installs.</p>"},{"location":"2024/04/06/setting-up-a-new-windows-laptop-for-python-development/#setting-up-on-windows","title":"Setting up on Windows","text":"<p>While writing this guide (mostly to myself), I ended up diving a bit into Windows package managers and which would be the most suitable. I've spun this out into a separate post so that I can keep this page focused on actual installation steps. But my conclusions were:</p> <ul> <li>use <code>scoop</code> for any Windows dev tools and a first port of call for other apps</li> <li>use <code>winget</code> for Microsoft products e.g. OneDrive, OneNote, etc.</li> <li>use <code>choco</code> as a back up for apps that aren't available in <code>scoop</code> e.g. Docker Desktop</li> </ul> <p>So here we go on the Windows steps:</p>"},{"location":"2024/04/06/setting-up-a-new-windows-laptop-for-python-development/#updating-powershell-to-latest-version","title":"Updating Powershell to latest version","text":"<p>In Powershell with admin privileges:</p> <pre><code>winget install --id Microsoft.Powershell --source winget\n</code></pre>"},{"location":"2024/04/06/setting-up-a-new-windows-laptop-for-python-development/#install-microsoft-apps-with-winget","title":"Install Microsoft Apps with <code>winget</code>","text":"<p>In Powershell with admin privileges:</p> <pre><code>winget install Microsoft.OneDrive --accept-source-agreements --accept-package-agreements\nwinget install onenote --accept-source-agreements --accept-package-agreements\n</code></pre> <p>Note</p> <p>Adding the arguments <code>--accept-source-agreements --accept-package-agreements</code> results in less confirmations needed in the shell. Use if you're happy with it.</p>"},{"location":"2024/04/06/setting-up-a-new-windows-laptop-for-python-development/#installing-chocolatey","title":"Installing Chocolatey","text":"<p>In Powershell with admin privileges:</p> <pre><code>Set-ExecutionPolicy AllSigned; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://community.chocolatey.org/install.ps1'))\n</code></pre> <p>Confirm the prompts as necessary (trying to figure out a way to skip these temporarily). You should now have Chocolatey installed, you can check by running <code>choco</code> from the command line.</p> <p>Warning</p> <p>The install guide says to inspect their <code>install.ps1</code> script before running, even though they take security seriously. Personally, I'm happy to go ahead with it and I'm not quite sure what I'm looking for anyway.</p>"},{"location":"2024/04/06/setting-up-a-new-windows-laptop-for-python-development/#installing-scoop","title":"Installing <code>scoop</code>","text":"<p>In the <code>scoop</code> docs, it says to install from a Powershell terminal with the following:</p> <pre><code>Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser\nInvoke-RestMethod -Uri https://get.scoop.sh | Invoke-Expression\n</code></pre> <p>However, I was getting a few issues with the execution policy - this seemed to work instead:</p> <pre><code>Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser\n[Net.ServicePointManager]::SecurityProtocol = [Net.SecurityProtocolType]::Tls12 -bor [Net.SecurityProtocolType]::Tls11 -bor [Net.SecurityProtocolType]::Tls\n$AllProtocols = [System.Net.SecurityProtocolType]'Ssl3,Tls,Tls11,Tls12'\n[System.Net.ServicePointManager]::SecurityProtocol = $AllProtocols\n\nirm get.scoop.sh | iex\n</code></pre> <p>This issues may be down to the version of Powershell I was on when initially installing <code>scoop</code> but this remains uninvestigated - just need something that works.</p>"},{"location":"2024/04/06/setting-up-a-new-windows-laptop-for-python-development/#installing-some-apps-with-scoop","title":"Installing some apps with scoop","text":"<p>I just wanted to touch briefly on a few apps installed here.</p> <p>Particularly for develepment I'm installing:</p> <ul> <li><code>git</code> as my version control system</li> <li><code>pyenv</code> so that I can install multiple Python versions in isolated environments and easily switch   between them. <code>scoop</code> will install <code>pyenv-win</code> since it is an installer for Windows.</li> <li>VS Code as my main IDE</li> <li>Oh My Posh which offers nice prompt customisation.</li> </ul> <p>Installing a couple of chat tools:</p> <ul> <li>Slack: some developer communities are on here.</li> <li>Element: same here - I've been communicating with the <code>dyanconf</code>   maintainers here recently on some recent contributions.</li> </ul> <p>And then I've just got Firefox as my browser, Obsidian which is a note-taking app I am experimenting with a little. And Steam, because... well...</p> <p>In any shell (doesn't need to be admin):</p> <pre><code>scoop update\nscoop install git\nscoop install pyenv\nscoop install https://github.com/JanDeDobbeleer/oh-my-posh/releases/latest/download/oh-my-posh.json\nscoop bucket add extras\nscoop install vscode\nreg import \"C:\\Users\\User\\scoop\\apps\\vscode\\current\\install-context.reg\"\nreg import \"C:\\Users\\User\\scoop\\apps\\vscode\\current\\install-associations.reg\"\n# I think the following 3 are in extras too, since they have GUIs.\nscoop install screentogif\nscoop install obsidian\nscoop install element\nscoop install slack\nscoop install firefox\nscoop bucket add games\nscoop install steam\n</code></pre>"},{"location":"2024/04/06/setting-up-a-new-windows-laptop-for-python-development/#installing-some-apps-with-chocolatey","title":"Installing some apps with Chocolatey","text":"<p>There were a couple of apps that I struggled to install via <code>scoop</code>:</p> <ul> <li>Docker Desktop: this is another developer tool for pulling and running   containers, which happens to integrate very nicely with WSL.</li> <li>Discord: another chat app with developer communities, plus I use this to chat to my rage buddies.</li> </ul> <p>You can install <code>docker</code> with scoop since it's just a CLI tool, but I wanted the Desktop GUI too. For some reason it's not in the extras bucket but that's fine, it's why I have <code>choco</code> as backup.</p> <p>Discord is available on <code>scoop</code>, but for some reason when I installed it via scoop Windows Defender seemed to think it was a trojan. Sounds like I should be panicking huh? But a quick search seemed to identify it as a known false flag - panic over?. Anyway, I just decided to use <code>choco</code> for this rather than investigate further.</p> <p>In Powershell with admin privileges:</p> <pre><code>choco install docker-desktop\nchoco install discord\n</code></pre> <p>Note</p> <p>If you don't want to confirm Y/N for each install, you can switch this off in choco global config with <code>choco feature enable -n=allowGlobalConfirmation</code>.</p>"},{"location":"2024/04/06/setting-up-a-new-windows-laptop-for-python-development/#installing-python","title":"Installing Python","text":"<p>As noted in my setup goals the plan is to have WSL as my main development environment, but it will still be useful to have at least one version of Python installed on the Windows path. I'll use <code>pyenv</code> to install it.</p> <p>In any shell (doesn't need to be admin):</p> <pre><code>pyenv update # (2)!\n# Install the latest Python version\nlatest_py=$(pyenv install --list | grep -v '[a-z]' | tail -1) # (1)!\npyenv install $latest_py\npyenv global $latest_py # (3)!\n</code></pre> <ol> <li>This will grab the latest stable Python release.</li> <li>Make sure pyenv is up-to-date with current versions.</li> <li>Once installed we'll set this version as the global default.</li> </ol>"},{"location":"2024/04/06/setting-up-a-new-windows-laptop-for-python-development/#setting-up-vs-code","title":"Setting up VS Code","text":"<p>This is quite easy with VS Code's settings sync feature which I'd switched on and linked to my GitHub account on my old PC. All I had to do was log in and all of my configurations, extensions and keybindings were ported like magic.</p> <p></p> <p>This guide is already getting quite long so I won't get into the specifics of my VS Code setup just yet, maybe another time...</p>"},{"location":"2024/04/06/setting-up-a-new-windows-laptop-for-python-development/#setting-up-on-linux-wsl","title":"Setting up on Linux (WSL)","text":"<p>I've put up a separate post covering how I've setup my WSL development environment in a portable way. It's definitely worth checking out!</p>"},{"location":"2024/04/06/setting-up-a-new-windows-laptop-for-python-development/#extras","title":"Extras","text":""},{"location":"2024/04/06/setting-up-a-new-windows-laptop-for-python-development/#make-sure-docker-desktop-configured-to-wsl","title":"Make sure Docker Desktop configured to WSL","text":"<p>Make sure this setting is checked within Docker Desktop to use with WSL. This should make the Docker CLI available in WSL while Docker Desktop is up and running.</p> <p></p>"},{"location":"2024/04/06/setting-up-a-new-windows-laptop-for-python-development/#custom-terminal-prompts-with-oh-my-posh","title":"Custom terminal prompts with Oh-My-Posh","text":"<p>Oh My Posh brings a nice pop of colour to the terminal with a variety of themes to choose from. Check out all of the fully customisable themes here.</p> Installing with scoop...<pre><code>scoop install https://github.com/JanDeDobbeleer/oh-my-posh/releases/latest/download/oh-my-posh.json\n$env:Path += \";C:\\Users\\user\\AppData\\Local\\Programs\\oh-my-posh\\bin\"\n</code></pre> <p>Many of the prompt themes make use of iconography which requires you to have a Nerd Font installed. Oh My Posh provides a nice tool to install the one you want:</p> <pre><code>oh-my-posh font install FiraCode\n</code></pre> <p>Because I want to configure the Nerd Fonts for Windows Terminal (which WSL launches in on Windows 11) and Visual Studio Code, I think I need the fonts installed on Windows rather than Linux. I installed in both though just to be sure:</p> <pre><code>oh-my-posh font install Hack\n</code></pre> <p>I had a bit of trouble configuring the VS Code font for a while, I think because I didn't have \"Mono\" in the font name. Anyway, here's the two settings that I've configured for the 'Hack' Nerd Font:</p> Windows Terminal settings.json<pre><code>    \"profiles\":\n    {\n        \"defaults\": {\n            \"font\":\n            {\n                \"face\": \"Hack Nerd Font Mono\"\n            }\n        },\n        ...\n    }\n</code></pre> VS Code settings.json<pre><code>  \"terminal.integrated.fontFamily\": \"'Hack Nerd Font Mono'\",\n</code></pre> <p>And with the font configured (only for nushell currently):</p> <p></p> <p>I've configured for Monty's theme here. You can also check out my other nu config in the <code>custom.nu</code> file there if you're interested.</p>"},{"location":"2024/04/06/setting-up-a-new-windows-laptop-for-python-development/#wrap-up","title":"Wrap Up","text":"<p>So there you have it, my setup for Python development and beyond on my new Windows 11 laptop. It's been a bit of a beast doing all of it and capturing in writing but I hope you (and my future self) gets something useful from it.</p> <p>I may come back and make some new additions to this from time-to-time as I integrate more into my workflow, but this is more than enough for now.</p> <p>Which parts of this guide will you try and integrate into your own environment?</p>"},{"location":"archive/2024/","title":"2024","text":""},{"location":"category/linux/","title":"linux","text":""},{"location":"category/bash/","title":"bash","text":""},{"location":"category/python/","title":"python","text":""},{"location":"category/windows/","title":"windows","text":""},{"location":"category/wsl/","title":"wsl","text":""},{"location":"category/devtools/","title":"devtools","text":""},{"location":"category/pre-commit/","title":"pre-commit","text":""},{"location":"category/git/","title":"git","text":""},{"location":"category/tips/","title":"tips","text":""},{"location":"category/documentation/","title":"documentation","text":""},{"location":"category/mkdocs/","title":"mkdocs","text":""}]}